{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watershed Distance Transform for 2D Data\n",
    "---\n",
    "Implementation of papers:\n",
    "\n",
    "[Deep Watershed Transform for Instance Segmentation](http://openaccess.thecvf.com/content_cvpr_2017/papers/Bai_Deep_Watershed_Transform_CVPR_2017_paper.pdf)\n",
    "\n",
    "[Learn to segment single cells with deep distance estimator and deep cell detector](https://arxiv.org/abs/1803.10829)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/deepcell/utils/__init__.py:49: UserWarning: To use `compute_overlap`, the C extensions must be built using `python setup.py build_ext --inplace`\n",
      "  warnings.warn('To use `compute_overlap`, the C extensions must be built '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell\n",
    "from tensorflow.python import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "n_epoch = 9  # Number of training epochs\n",
    "test_size = .10  # % of data saved as test\n",
    "norm_method = 'std'  # data normalization\n",
    "receptive_field = 81  # should be adjusted for the scale of the data\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "\n",
    "# Sample mode settings\n",
    "batch_size = 64  # number of images per batch (should be 2 ^ n)\n",
    "win = (receptive_field - 1) // 2  # sample window size\n",
    "balance_classes = True  # sample each class equally\n",
    "max_class_samples = 1e6  # max number of samples per class\n",
    "\n",
    "# Transformation settings\n",
    "transform = 'watershed'\n",
    "distance_bins = 4  # number of distance \"classes\"\n",
    "erosion_width = 1  # erode edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = 'Decidua_Object_Train_Small_Nuclei'\n",
    "npz_name = '/data/npz_data/' + base_name + \".npz\"\n",
    "MODEL_DIR = '/data/models/' + '20190808_decidua_small_nuclei'\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "LOG_DIR = '/data/logs'\n",
    "\n",
    "sample_model_name = base_name + \"_watershed_81_rf_64_conv_256_dense_300k_5bins\"\n",
    "fgbg_model_name = base_name + \"_watershed_fgbg_81_rf_64_conv_256_dense_300k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (5, 1024, 1024, 6) & y.shape: (5, 1024, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "CHANNEL_AXIS = 3\n",
    "training_data = np.load(npz_name)\n",
    "\n",
    "X, y = training_data[\"X\"], training_data[\"y\"]\n",
    "print(\"X.shape: {} & y.shape: {}\".format(X.shape, y.shape))\n",
    "#X = X[:, :, :, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, create a foreground/background separation model\n",
    "\n",
    "#### Instantiate the fgbg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "fgbg_model = model_zoo.bn_feature_net_2D(\n",
    "    receptive_field=receptive_field,\n",
    "    n_channels=X.shape[CHANNEL_AXIS],\n",
    "    n_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all data as training data\n",
      "Using class weights of {0: 1.0, 1: 1.0}\n",
      "X_train shape: (5, 1024, 1024, 6)\n",
      "y_train shape: (5, 1024, 1024, 1)\n",
      "Output Shape: (None, 2)\n",
      "Number of Classes: 2\n",
      "Training on 1 GPUs\n",
      "the max_class_samples per image is 200000\n",
      "analyzing image 0\n",
      "the least represented class has 259505 examples\n",
      "max_class_samples is less than the smalleset class, downsampling all classes\n",
      "analyzing class 0\n",
      "downsampling from 259505 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 629744 examples per class\n",
      "analyzing image 1\n",
      "the least represented class has 226163 examples\n",
      "max_class_samples is less than the smalleset class, downsampling all classes\n",
      "analyzing class 0\n",
      "downsampling from 226163 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 663086 examples per class\n",
      "analyzing image 2\n",
      "the least represented class has 269108 examples\n",
      "max_class_samples is less than the smalleset class, downsampling all classes\n",
      "analyzing class 0\n",
      "downsampling from 620141 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 269108 examples per class\n",
      "analyzing image 3\n",
      "the least represented class has 217342 examples\n",
      "max_class_samples is less than the smalleset class, downsampling all classes\n",
      "analyzing class 0\n",
      "downsampling from 671907 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 217342 examples per class\n",
      "analyzing image 4\n",
      "the least represented class has 253701 examples\n",
      "max_class_samples is less than the smalleset class, downsampling all classes\n",
      "analyzing class 0\n",
      "downsampling from 253701 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 635548 examples per class\n",
      "running model without validation checks\n",
      "Epoch 1/5\n",
      "15624/15625 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.8669\n",
      "Epoch 00001: saving model to /data/models/20190808_decidua_small_nuclei/Decidua_Object_Train_Small_Nuclei_watershed_fgbg_81_rf_64_conv_256_dense_300k_epoch_01.h5\n",
      "15625/15625 [==============================] - 8870s 568ms/step - loss: 0.3085 - acc: 0.8669\n",
      "Epoch 2/5\n",
      "12867/15625 [=======================>......] - ETA: 26:02 - loss: 0.2716 - acc: 0.8845"
     ]
    }
   ],
   "source": [
    "from deepcell.training import train_model_sample\n",
    "\n",
    "fgbg_model = train_model_sample(\n",
    "    model=fgbg_model,\n",
    "    dataset=npz_name,\n",
    "    model_name=fgbg_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    window_size=(win, win),\n",
    "    batch_size=128,\n",
    "    transform='fgbg',\n",
    "    n_epoch=5,\n",
    "    balance_classes=balance_classes,\n",
    "    max_class_samples=1000000,\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    val_monitor=False,\n",
    "    save_period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the distance transform model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "from deepcell.training import train_model_sample\n",
    "\n",
    "watershed_model = model_zoo.bn_feature_net_2D(\n",
    "    receptive_field=receptive_field,\n",
    "    n_channels=X.shape[CHANNEL_AXIS],\n",
    "    n_features=distance_bins,\n",
    "    n_conv_filters=64,\n",
    "    n_dense_filters=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Decidua_Object_Train_Small_Nuclei_watershed_81_rf_64_conv_256_dense_300k_5bins'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model_name\n",
    "#watershed_model.load_weights('/data/models/20190606_params/Point1_12_18_3X_interior_border_border_watershed_128_filters_balanced_epoch_40.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all data as training data\n",
      "Using class weights of {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "X_train shape: (5, 1024, 1024, 6)\n",
      "y_train shape: (5, 1024, 1024, 1)\n",
      "Output Shape: (None, 5)\n",
      "Number of Classes: 5\n",
      "Training on 1 GPUs\n",
      "the max_class_samples per image is 60000\n",
      "analyzing image 0\n",
      "the least represented class has 35379 examples\n",
      "analyzing class 0\n",
      "downsampling from 489607 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 162204 examples per class\n",
      "analyzing class 2\n",
      "downsampling from 121323 examples per class\n",
      "analyzing class 3\n",
      "downsampling from 80736 examples per class\n",
      "analyzing class 4\n",
      "downsampling from 35379 examples per class\n",
      "analyzing image 1\n",
      "the least represented class has 36272 examples\n",
      "analyzing class 0\n",
      "downsampling from 469630 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 172043 examples per class\n",
      "analyzing class 2\n",
      "downsampling from 127060 examples per class\n",
      "analyzing class 3\n",
      "downsampling from 84244 examples per class\n",
      "analyzing class 4\n",
      "downsampling from 36272 examples per class\n",
      "analyzing image 2\n",
      "the least represented class has 16195 examples\n",
      "analyzing class 0\n",
      "downsampling from 718568 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 68062 examples per class\n",
      "analyzing class 2\n",
      "downsampling from 51927 examples per class\n",
      "analyzing class 3\n",
      "downsampling from 34497 examples per class\n",
      "analyzing class 4\n",
      "downsampling from 16195 examples per class\n",
      "analyzing image 3\n",
      "the least represented class has 12152 examples\n",
      "analyzing class 0\n",
      "downsampling from 753299 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 55121 examples per class\n",
      "analyzing class 2\n",
      "downsampling from 41435 examples per class\n",
      "analyzing class 3\n",
      "downsampling from 27242 examples per class\n",
      "analyzing class 4\n",
      "downsampling from 12152 examples per class\n",
      "analyzing image 4\n",
      "the least represented class has 35137 examples\n",
      "analyzing class 0\n",
      "downsampling from 491289 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 162586 examples per class\n",
      "analyzing class 2\n",
      "downsampling from 121164 examples per class\n",
      "analyzing class 3\n",
      "downsampling from 79073 examples per class\n",
      "analyzing class 4\n",
      "downsampling from 35137 examples per class\n",
      "running model without validation checks\n",
      "Epoch 1/9\n",
      "  157/10557 [..............................] - ETA: 52:02 - loss: 1.4269 - acc: 0.3246"
     ]
    }
   ],
   "source": [
    "watershed_model = train_model_sample(\n",
    "    model=watershed_model,\n",
    "    dataset=npz_name, \n",
    "    model_name=sample_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=batch_size,\n",
    "    n_epoch=9,\n",
    "    window_size=(win, win),\n",
    "    transform=transform,\n",
    "    distance_bins=distance_bins,\n",
    "    erosion_width=erosion_width,\n",
    "    balance_classes=True,\n",
    "    max_class_samples=300000,\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    val_monitor=False,\n",
    "    save_period=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
