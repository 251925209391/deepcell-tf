{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watershed Distance Transform for 2D Data\n",
    "---\n",
    "Implementation of papers:\n",
    "\n",
    "[Deep Watershed Transform for Instance Segmentation](http://openaccess.thecvf.com/content_cvpr_2017/papers/Bai_Deep_Watershed_Transform_CVPR_2017_paper.pdf)\n",
    "\n",
    "[Learn to segment single cells with deep distance estimator and deep cell detector](https://arxiv.org/abs/1803.10829)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/deepcell/utils/__init__.py:49: UserWarning: To use `compute_overlap`, the C extensions must be built using `python setup.py build_ext --inplace`\n",
      "  warnings.warn('To use `compute_overlap`, the C extensions must be built '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "n_epoch = 40  # Number of training epochs\n",
    "test_size = .10  # % of data saved as test\n",
    "norm_method = 'std'  # data normalization\n",
    "receptive_field = 61  # should be adjusted for the scale of the data\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "\n",
    "# Sample mode settings\n",
    "batch_size = 64  # number of images per batch (should be 2 ^ n)\n",
    "win = (receptive_field - 1) // 2  # sample window size\n",
    "balance_classes = True  # sample each class equally\n",
    "max_class_samples = 1e6  # max number of samples per class\n",
    "\n",
    "# Transformation settings\n",
    "transform = 'watershed'\n",
    "distance_bins = 4  # number of distance \"classes\"\n",
    "erosion_width = 0  # erode edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = 'Point1_12_18_3X_interior_border_border'\n",
    "npz_name = '/data/npz_data/' + base_name + \".npz\"\n",
    "MODEL_DIR = '/data/models/' + '20190525_class_balance'\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "LOG_DIR = '/data/logs'\n",
    "\n",
    "\n",
    "fgbg_model_name = base_name + \"_fgbg\"\n",
    "sample_model_name = base_name + \"_watershed_balanced\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (3, 1024, 1024, 3) & y.shape: (3, 1024, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "CHANNEL_AXIS = 3\n",
    "training_data = np.load(npz_name)\n",
    "\n",
    "X, y = training_data[\"X\"], training_data[\"y\"]\n",
    "print(\"X.shape: {} & y.shape: {}\".format(X.shape, y.shape))\n",
    "#X = X[:, :, :, 0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, create a foreground/background separation model\n",
    "\n",
    "#### Instantiate the fgbg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "fgbg_model = model_zoo.bn_feature_net_2D(\n",
    "    receptive_field=receptive_field,\n",
    "    n_channels=X.shape[CHANNEL_AXIS],\n",
    "    n_features=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the fgbg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all data as training data\n",
      "Using class weights of {0: 1.0, 1: 1.0}\n",
      "X_train shape: (3, 1024, 1024, 1)\n",
      "y_train shape: (3, 1024, 1024, 1)\n",
      "Output Shape: (None, 2)\n",
      "Number of Classes: 2\n",
      "Training on 1 GPUs\n",
      "the max_class_samples per image is 333333\n",
      "analyzing image 0\n",
      "the least represented class has 331425 examples\n",
      "analyzing class 0\n",
      "downsampling from 595944 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 331425 examples per class\n",
      "analyzing image 1\n",
      "the least represented class has 421168 examples\n",
      "max_class_samples is less than the smalleset class, downsampling all classes\n",
      "analyzing class 0\n",
      "downsampling from 506201 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 421168 examples per class\n",
      "analyzing image 2\n",
      "the least represented class has 434941 examples\n",
      "max_class_samples is less than the smalleset class, downsampling all classes\n",
      "analyzing class 0\n",
      "downsampling from 434941 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 492428 examples per class\n",
      "running model without validation checks\n",
      "Epoch 1/5\n",
      "15594/15595 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9168\n",
      "Epoch 00001: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_fgbg_epoch_01.h5\n",
      "15595/15595 [==============================] - 1448s 93ms/step - loss: 0.1966 - acc: 0.9168\n",
      "Epoch 2/5\n",
      "15594/15595 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9190\n",
      "Epoch 00002: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_fgbg_epoch_02.h5\n",
      "15595/15595 [==============================] - 1441s 92ms/step - loss: 0.1911 - acc: 0.9190\n",
      "Epoch 3/5\n",
      "15594/15595 [============================>.] - ETA: 0s - loss: 0.1886 - acc: 0.9199\n",
      "Epoch 00003: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_fgbg_epoch_03.h5\n",
      "15595/15595 [==============================] - 1440s 92ms/step - loss: 0.1886 - acc: 0.9199\n",
      "Epoch 4/5\n",
      "15594/15595 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9208\n",
      "Epoch 00004: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_fgbg_epoch_04.h5\n",
      "15595/15595 [==============================] - 1402s 90ms/step - loss: 0.1861 - acc: 0.9208\n",
      "Epoch 5/5\n",
      "15594/15595 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9217\n",
      "Epoch 00005: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_fgbg_epoch_05.h5\n",
      "15595/15595 [==============================] - 1419s 91ms/step - loss: 0.1843 - acc: 0.9217\n"
     ]
    }
   ],
   "source": [
    "from deepcell.training import train_model_sample\n",
    "\n",
    "fgbg_model = train_model_sample(\n",
    "    model=fgbg_model,\n",
    "    dataset=npz_name,\n",
    "    model_name=fgbg_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    window_size=(win, win),\n",
    "    batch_size=128,\n",
    "    transform='fgbg',\n",
    "    n_epoch=5,\n",
    "    balance_classes=balance_classes,\n",
    "    max_class_samples=1000000,\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    val_monitor=False,\n",
    "    save_period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, Create a model for the watershed energy transform\n",
    "\n",
    "#### Instantiate the distance transform model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "from deepcell.training import train_model_sample\n",
    "\n",
    "watershed_model = model_zoo.bn_feature_net_2D(\n",
    "    receptive_field=receptive_field,\n",
    "    n_channels=X.shape[CHANNEL_AXIS],\n",
    "    n_features=distance_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Point1_12_18_3X_interior_border_border_watershed_balanced'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all data as training data\n",
      "Using class weights of {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0}\n",
      "X_train shape: (3, 1024, 1024, 3)\n",
      "y_train shape: (3, 1024, 1024, 1)\n",
      "Output Shape: (None, 4)\n",
      "Number of Classes: 4\n",
      "Training on 1 GPUs\n",
      "the max_class_samples per image is 110000\n",
      "analyzing image 0\n",
      "the least represented class has 40925 examples\n",
      "analyzing class 0\n",
      "downsampling from 691233 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 113666 examples per class\n",
      "analyzing class 2\n",
      "downsampling from 81545 examples per class\n",
      "analyzing class 3\n",
      "downsampling from 40925 examples per class\n",
      "analyzing image 1\n",
      "the least represented class has 48170 examples\n",
      "analyzing class 0\n",
      "downsampling from 642587 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 136247 examples per class\n",
      "analyzing class 2\n",
      "downsampling from 100365 examples per class\n",
      "analyzing class 3\n",
      "downsampling from 48170 examples per class\n",
      "analyzing image 2\n",
      "the least represented class has 62636 examples\n",
      "analyzing class 0\n",
      "downsampling from 579299 examples per class\n",
      "analyzing class 1\n",
      "downsampling from 159104 examples per class\n",
      "analyzing class 2\n",
      "downsampling from 126330 examples per class\n",
      "analyzing class 3\n",
      "downsampling from 62636 examples per class\n",
      "running model without validation checks\n",
      "Epoch 1/40\n",
      "9483/9483 [==============================] - 972s 102ms/step - loss: 0.6873 - acc: 0.6683\n",
      "Epoch 2/40\n",
      "9483/9483 [==============================] - 954s 101ms/step - loss: 0.6448 - acc: 0.6892\n",
      "Epoch 3/40\n",
      "9483/9483 [==============================] - 951s 100ms/step - loss: 0.6317 - acc: 0.6959\n",
      "Epoch 4/40\n",
      "9483/9483 [==============================] - 972s 103ms/step - loss: 0.6230 - acc: 0.7004\n",
      "Epoch 5/40\n",
      "9482/9483 [============================>.] - ETA: 0s - loss: 0.6148 - acc: 0.7042\n",
      "Epoch 00005: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_border_border_watershed_balanced_epoch_05.h5\n",
      "9483/9483 [==============================] - 969s 102ms/step - loss: 0.6148 - acc: 0.7042\n",
      "Epoch 6/40\n",
      "9483/9483 [==============================] - 990s 104ms/step - loss: 0.6084 - acc: 0.7085\n",
      "Epoch 7/40\n",
      "9483/9483 [==============================] - 974s 103ms/step - loss: 0.5958 - acc: 0.7158\n",
      "Epoch 8/40\n",
      "9483/9483 [==============================] - 990s 104ms/step - loss: 0.5667 - acc: 0.7323\n",
      "Epoch 9/40\n",
      "9483/9483 [==============================] - 984s 104ms/step - loss: 0.5394 - acc: 0.7485\n",
      "Epoch 10/40\n",
      "9482/9483 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.7567\n",
      "Epoch 00010: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_border_border_watershed_balanced_epoch_10.h5\n",
      "9483/9483 [==============================] - 990s 104ms/step - loss: 0.5240 - acc: 0.7567\n",
      "Epoch 11/40\n",
      "9483/9483 [==============================] - 985s 104ms/step - loss: 0.5133 - acc: 0.7624\n",
      "Epoch 12/40\n",
      "9483/9483 [==============================] - 983s 104ms/step - loss: 0.5069 - acc: 0.7658\n",
      "Epoch 13/40\n",
      "9483/9483 [==============================] - 983s 104ms/step - loss: 0.4987 - acc: 0.7701\n",
      "Epoch 14/40\n",
      "9483/9483 [==============================] - 984s 104ms/step - loss: 0.4938 - acc: 0.7730\n",
      "Epoch 15/40\n",
      "9482/9483 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.7760\n",
      "Epoch 00015: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_border_border_watershed_balanced_epoch_15.h5\n",
      "9483/9483 [==============================] - 982s 104ms/step - loss: 0.4891 - acc: 0.7760\n",
      "Epoch 16/40\n",
      "9483/9483 [==============================] - 990s 104ms/step - loss: 0.4830 - acc: 0.7787\n",
      "Epoch 17/40\n",
      "9483/9483 [==============================] - 982s 104ms/step - loss: 0.4792 - acc: 0.7804\n",
      "Epoch 18/40\n",
      "9483/9483 [==============================] - 985s 104ms/step - loss: 0.4744 - acc: 0.7826\n",
      "Epoch 19/40\n",
      "9483/9483 [==============================] - 985s 104ms/step - loss: 0.4719 - acc: 0.7841\n",
      "Epoch 20/40\n",
      "9482/9483 [============================>.] - ETA: 0s - loss: 0.4694 - acc: 0.7853\n",
      "Epoch 00020: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_border_border_watershed_balanced_epoch_20.h5\n",
      "9483/9483 [==============================] - 986s 104ms/step - loss: 0.4694 - acc: 0.7853\n",
      "Epoch 21/40\n",
      "9483/9483 [==============================] - 984s 104ms/step - loss: 0.4665 - acc: 0.7880\n",
      "Epoch 22/40\n",
      "9483/9483 [==============================] - 990s 104ms/step - loss: 0.4638 - acc: 0.7887\n",
      "Epoch 23/40\n",
      "9483/9483 [==============================] - 991s 105ms/step - loss: 0.4607 - acc: 0.7905\n",
      "Epoch 24/40\n",
      "9483/9483 [==============================] - 988s 104ms/step - loss: 0.4583 - acc: 0.7914\n",
      "Epoch 25/40\n",
      "9482/9483 [============================>.] - ETA: 0s - loss: 0.4561 - acc: 0.7927\n",
      "Epoch 00025: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_border_border_watershed_balanced_epoch_25.h5\n",
      "9483/9483 [==============================] - 986s 104ms/step - loss: 0.4561 - acc: 0.7927\n",
      "Epoch 26/40\n",
      "9483/9483 [==============================] - 1077s 114ms/step - loss: 0.4544 - acc: 0.7934\n",
      "Epoch 27/40\n",
      "9483/9483 [==============================] - 1015s 107ms/step - loss: 0.4514 - acc: 0.7950\n",
      "Epoch 28/40\n",
      "9483/9483 [==============================] - 993s 105ms/step - loss: 0.4503 - acc: 0.7957\n",
      "Epoch 29/40\n",
      "9483/9483 [==============================] - 992s 105ms/step - loss: 0.4485 - acc: 0.7967\n",
      "Epoch 30/40\n",
      "9482/9483 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.7978\n",
      "Epoch 00030: saving model to /data/models/20190525_class_balance/Point1_12_18_3X_interior_border_border_watershed_balanced_epoch_30.h5\n",
      "9483/9483 [==============================] - 1000s 105ms/step - loss: 0.4461 - acc: 0.7978\n",
      "Epoch 31/40\n",
      "9483/9483 [==============================] - 1004s 106ms/step - loss: 0.4453 - acc: 0.7981\n",
      "Epoch 32/40\n",
      "9483/9483 [==============================] - 1006s 106ms/step - loss: 0.4425 - acc: 0.7997\n",
      "Epoch 33/40\n",
      "9483/9483 [==============================] - 1002s 106ms/step - loss: 0.4423 - acc: 0.8000\n",
      "Epoch 34/40\n",
      "2051/9483 [=====>........................] - ETA: 13:09 - loss: 0.4425 - acc: 0.7988"
     ]
    }
   ],
   "source": [
    "watershed_model = train_model_sample(\n",
    "    model=watershed_model,\n",
    "    dataset=npz_name, \n",
    "    model_name=sample_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=batch_size,\n",
    "    n_epoch=n_epoch,\n",
    "    window_size=(win, win),\n",
    "    transform=transform,\n",
    "    distance_bins=distance_bins,\n",
    "    erosion_width=erosion_width,\n",
    "    balance_classes=True,\n",
    "    max_class_samples=330000,\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    val_monitor=False,\n",
    "    save_period=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
