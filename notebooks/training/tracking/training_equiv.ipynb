{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import errno\n",
    "import deepcell\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from deepcell.data.tracking import prepare_dataset\n",
    "from deepcell.data.tracking import Track, concat_tracks\n",
    "from deepcell.model_zoo.tracking import GNNTrackingModel\n",
    "from deepcell.utils.tracking_utils import trks_stats, load_trks\n",
    "\n",
    "from tensorflow_addons.optimizers import RectifiedAdam as RAdam\n",
    "\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1 GPUs\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU count\n",
    "from deepcell import train_utils\n",
    "num_gpus = train_utils.count_gpus()\n",
    "print('Training on {} GPUs'.format(num_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup directories\n",
    "ROOT_DIR = '/data'  # TODO: Change this! Usually a mounted volume\n",
    "\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models'))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs'))\n",
    "DATA_DIR = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "OUTPUT_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'nuc_tracking'))\n",
    "\n",
    "# create directories if they do not exist\n",
    "for d in (MODEL_DIR, LOG_DIR, OUTPUT_DIR):\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and view stats on this file\n",
    "filename = 'train.trks'\n",
    "path = os.path.join('../trk_data/',filename)\n",
    "trks_data = load_trks(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tracks = Track(tracked_data=trks_data)\n",
    "# track_info = concat_tracks([all_tracks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = os.path.abspath(os.path.join(ROOT_DIR, 'dataset_idxs_dvc.npy'))\n",
    "dataset_indicies = np.load(dataset_sizes, allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for metrics\n",
    "\"\"\"\n",
    "def filter_and_flatten(y_true, y_pred):\n",
    "    n_classes = tf.shape(y_true)[-1]\n",
    "    new_shape = [-1, n_classes]\n",
    "    y_true = tf.reshape(y_true, new_shape)\n",
    "    y_pred = tf.reshape(y_pred, new_shape)\n",
    "\n",
    "    # Mask out the padded cells\n",
    "    y_true_reduced = tf.reduce_sum(y_true, axis=-1)\n",
    "    good_loc = tf.where(y_true_reduced == 1)[:, 0]\n",
    "\n",
    "    y_true = tf.gather(y_true, good_loc, axis=0)\n",
    "    y_pred = tf.gather(y_pred, good_loc, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "class Recall(tf.keras.metrics.Recall):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Recall, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class Precision(tf.keras.metrics.Precision):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Precision, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "    return deepcell.losses.weighted_categorical_crossentropy(\n",
    "        y_true, y_pred,\n",
    "        n_classes=tf.shape(y_true)[-1],\n",
    "        axis=-1)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = RAdam(learning_rate=1e-3, clipnorm=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "losses = {'temporal_adj_matrices': loss_function}\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    Recall(class_id=0, name='same_recall'),\n",
    "    Recall(class_id=1, name='different_recall'),\n",
    "    Recall(class_id=2, name='daughter_recall'),\n",
    "    Precision(class_id=0, name='same_precision'),\n",
    "    Precision(class_id=1, name='different_precision'),\n",
    "    Precision(class_id=2, name='daughter_precision'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data idx 0 size 22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:54<00:00,  2.46s/it]\n",
      "100%|██████████| 22/22 [00:50<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "seed = 1   # random seed for training/validation data split\n",
    "batch_size = 4\n",
    "track_length = 8  # only train on 8 frames at once\n",
    "val_size = .20  # % of data saved as validation\n",
    "test_size = .1  # % of data held out as a test set\n",
    "n_epochs = 1  # number of training epochs\n",
    "\n",
    "# steps_per_epoch = 1000\n",
    "# validation_steps = 200\n",
    "steps_per_epoch = 10\n",
    "validation_steps = 2\n",
    "\n",
    "translation_range = 512 #X_train.shape[-2]\n",
    "\n",
    "n_layers = 1 # number of graph convolutions\n",
    "\n",
    "\n",
    "# for i in range(len(dataset_indicies)):\n",
    "i=0\n",
    "new_data = {}\n",
    "new_data['lineages'] = list(np.array(trks_data['lineages'])[dataset_indicies[i]])\n",
    "new_data['X'] = trks_data['X'][dataset_indicies[i],...]\n",
    "new_data['y'] = trks_data['y'][dataset_indicies[i],...]\n",
    "\n",
    "ds_size = len(dataset_indicies[i])\n",
    "\n",
    "print()\n",
    "print('data idx', i, 'size', ds_size)\n",
    "print()\n",
    "\n",
    "all_tracks = Track(tracked_data=new_data)\n",
    "track_info = concat_tracks([all_tracks])\n",
    "\n",
    "# find maximum number of cells in any frame\n",
    "max_cells = track_info['appearances'].shape[2]\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = prepare_dataset(\n",
    "    track_info,\n",
    "    rotation_range=180,\n",
    "    translation_range=translation_range,\n",
    "    seed=seed,\n",
    "    val_size=val_size,\n",
    "    test_size=test_size,\n",
    "    batch_size=batch_size,\n",
    "    track_length=track_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "# seed = 1   # random seed for training/validation data split\n",
    "# batch_size = 4\n",
    "# track_length = 8  # only train on 8 frames at once\n",
    "# val_size = .20  # % of data saved as validation\n",
    "# test_size = .1  # % of data held out as a test set\n",
    "# n_epochs = 1  # number of training epochs\n",
    "\n",
    "# # steps_per_epoch = 1000\n",
    "# # validation_steps = 200\n",
    "# steps_per_epoch = 10\n",
    "# validation_steps = 2\n",
    "\n",
    "# translation_range = 512 #X_train.shape[-2]\n",
    "\n",
    "# n_layers = 1 # number of graph convolutions\n",
    "\n",
    "# model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "# model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "# # for i in range(len(dataset_indicies)):\n",
    "# i=0\n",
    "# new_data = {}\n",
    "# new_data['lineages'] = list(np.array(trks_data['lineages'])[dataset_indicies[i]])\n",
    "# new_data['X'] = trks_data['X'][dataset_indicies[i],...]\n",
    "# new_data['y'] = trks_data['y'][dataset_indicies[i],...]\n",
    "\n",
    "# ds_size = len(dataset_indicies[i])\n",
    "\n",
    "# print()\n",
    "# print('data idx', i, 'size', ds_size)\n",
    "# print()\n",
    "\n",
    "# all_tracks = Track(tracked_data=new_data)\n",
    "# track_info = concat_tracks([all_tracks])\n",
    "\n",
    "# # find maximum number of cells in any frame\n",
    "# max_cells = track_info['appearances'].shape[2]\n",
    "\n",
    "\n",
    "# model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "# model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "\n",
    "# train_data, val_data, test_data = prepare_dataset(\n",
    "#     track_info,\n",
    "#     rotation_range=180,\n",
    "#     translation_range=translation_range,\n",
    "#     seed=seed,\n",
    "#     val_size=val_size,\n",
    "#     test_size=test_size,\n",
    "#     batch_size=batch_size,\n",
    "#     track_length=track_length)\n",
    "\n",
    "model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "graph_layer = 'se2t'  # type of graph convolution layer\n",
    "# for graph_layer in ['se2t', 'gcn', 'se2c', 'gcs']:\n",
    "\n",
    "tm = GNNTrackingModel(max_cells=max_cells, n_layers=n_layers, graph_layer=graph_layer)\n",
    "\n",
    "# Compile model\n",
    "tm.training_model.compile(optimizer=optimizer, loss=losses, metrics=metrics)\n",
    "\n",
    "layer = graph_layer\n",
    "ds_size = 5\n",
    "\n",
    "# run = wandb.init(project='testing_new', reinit=True)\n",
    "# wandb.run.name = layer+f'_datasize_{ds_size}'\n",
    "\n",
    "# wandb.log({'metrics': metrics,\n",
    "#             'losses': losses})\n",
    "\n",
    "# Train the model\n",
    "train_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor='val_loss',\n",
    "        save_best_only=True, verbose=1,\n",
    "        save_weights_only=False),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7)]#, WandbCallback()]\n",
    "\n",
    "loss_history = tm.training_model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=train_callbacks)\n",
    "\n",
    "# Save models for prediction\n",
    "inf_path = os.path.join(MODEL_DIR, f'TrackingModelInf_{layer}_datasize_{ds_size}')\n",
    "ne_path = os.path.join(MODEL_DIR, f'TrackingModelNE_{layer}_datasize_{ds_size}')\n",
    "\n",
    "tm.inference_model.save(inf_path)\n",
    "tm.neighborhood_encoder.save(ne_path)\n",
    "\n",
    "print()\n",
    "print('finished this shit')\n",
    "print()\n",
    "\n",
    "# run.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1   # random seed for training/validation data split\n",
    "batch_size = 4\n",
    "track_length = 8  # only train on 8 frames at once\n",
    "val_size = .20  # % of data saved as validation\n",
    "test_size = .1  # % of data held out as a test set\n",
    "n_epochs = 12  # number of training epochs\n",
    "\n",
    "steps_per_epoch = 1000\n",
    "validation_steps = 200\n",
    "\n",
    "n_layers = 1  # number of graph convolutions\n",
    "# graph_layer = 'se2c'  # type of graph convolution layer\n",
    "\n",
    "translation_range = 512 #X_train.shape[-2]\n",
    "\n",
    "model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "model_path = os.path.join(MODEL_DIR, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_flatten(y_true, y_pred):\n",
    "    n_classes = tf.shape(y_true)[-1]\n",
    "    new_shape = [-1, n_classes]\n",
    "    y_true = tf.reshape(y_true, new_shape)\n",
    "    y_pred = tf.reshape(y_pred, new_shape)\n",
    "\n",
    "    # Mask out the padded cells\n",
    "    y_true_reduced = tf.reduce_sum(y_true, axis=-1)\n",
    "    good_loc = tf.where(y_true_reduced == 1)[:, 0]\n",
    "\n",
    "    y_true = tf.gather(y_true, good_loc, axis=0)\n",
    "    y_pred = tf.gather(y_pred, good_loc, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "class Recall(tf.keras.metrics.Recall):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Recall, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class Precision(tf.keras.metrics.Precision):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Precision, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "    return deepcell.losses.weighted_categorical_crossentropy(\n",
    "        y_true, y_pred,\n",
    "        n_classes=tf.shape(y_true)[-1],\n",
    "        axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.optimizers import RectifiedAdam as RAdam\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = RAdam(learning_rate=1e-3, clipnorm=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "losses = {'temporal_adj_matrices': loss_function}\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    Recall(class_id=0, name='same_recall'),\n",
    "    Recall(class_id=1, name='different_recall'),\n",
    "    Recall(class_id=2, name='daughter_recall'),\n",
    "    Precision(class_id=0, name='same_precision'),\n",
    "    Precision(class_id=1, name='different_precision'),\n",
    "    Precision(class_id=2, name='daughter_precision'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data idx 0 size 22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:46<00:00,  2.10s/it]\n",
      "100%|██████████| 22/22 [00:41<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "se2t\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jwa8261a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 253... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137c7d3e72cf4a4ca3d0f5a76c5d602f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">nightmarish-sorcery-4</strong>: <a href=\"https://wandb.ai/ulisrael/cell_tracking/runs/jwa8261a\" target=\"_blank\">https://wandb.ai/ulisrael/cell_tracking/runs/jwa8261a</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211031_001451-jwa8261a/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jwa8261a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ulisrael/cell_tracking/runs/1d6q4e7n\" target=\"_blank\">spooky-reaper-2</a></strong> to <a href=\"https://wandb.ai/ulisrael/cell_tracking\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset_indicies)):\n",
    "\n",
    "    new_data = {}\n",
    "    new_data['lineages'] = list(np.array(trks_data['lineages'])[dataset_indicies[i]])\n",
    "    new_data['X'] = trks_data['X'][dataset_indicies[i],...]\n",
    "    new_data['y'] = trks_data['y'][dataset_indicies[i],...]\n",
    "    \n",
    "    ds_size = len(dataset_indicies[i])\n",
    "\n",
    "    print()\n",
    "    print('data idx', i, 'size', ds_size)\n",
    "    print()\n",
    "\n",
    "    all_tracks = Track(tracked_data=new_data)\n",
    "    track_info = concat_tracks([all_tracks])\n",
    "\n",
    "    max_cells = track_info['appearances'].shape[2]\n",
    "\n",
    "    train_data, val_data, test_data = prepare_dataset(\n",
    "        track_info,\n",
    "        rotation_range=180,\n",
    "        translation_range=translation_range,\n",
    "        seed=seed,\n",
    "        val_size=val_size,\n",
    "        test_size=test_size,\n",
    "        batch_size=batch_size,\n",
    "        track_length=track_length)\n",
    "\n",
    "    graph_layers = ['se2t', 'gcn', 'se2c', 'gcs']\n",
    "    for layer in graph_layers:\n",
    "        print()\n",
    "        print(layer)\n",
    "        print()\n",
    "\n",
    "        run = wandb.init(project='cell_tracking', reinit=True)\n",
    "        wandb.run.name = layer+f'_datasize_{ds_size}'\n",
    "\n",
    "        tm = GNNTrackingModel(max_cells=max_cells, n_layers=n_layers, graph_layer=layer)\n",
    "\n",
    "        wandb.log({'metrics': metrics,\n",
    "                   'losses': losses})\n",
    "\n",
    "        tm.training_model.compile(optimizer=optimizer, loss=losses, metrics=metrics)\n",
    "\n",
    "        # Train the model\n",
    "        train_callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                model_path, monitor='val_loss',\n",
    "                save_best_only=True, verbose=1,\n",
    "                save_weights_only=False),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, verbose=1,\n",
    "                patience=3, min_lr=1e-7), WandbCallback()\n",
    "        ]\n",
    "\n",
    "        loss_history = tm.training_model.fit(\n",
    "            train_data,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=val_data,\n",
    "            validation_steps=validation_steps,\n",
    "            epochs=n_epochs,\n",
    "            verbose=1,\n",
    "            callbacks=train_callbacks)\n",
    "\n",
    "\n",
    "        # Save models for prediction\n",
    "        inf_path = os.path.join(MODEL_DIR, f'TrackingModelInf_{layer}_datasize_{ds_size}')\n",
    "        ne_path = os.path.join(MODEL_DIR, f'TrackingModelNE_{layer}_datasize_{ds_size}')\n",
    "\n",
    "        tm.inference_model.save(inf_path)\n",
    "        tm.neighborhood_encoder.save(ne_path)\n",
    "\n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Verify GPU count\n",
    "from deepcell import train_utils\n",
    "num_gpus = train_utils.count_gpus()\n",
    "print('Training on {} GPUs'.format(num_gpus))\n",
    "\n",
    "layer = graph_layer\n",
    "\n",
    "run = wandb.init(project='cell_tracking', reinit=True)\n",
    "wandb.run.name = layer+f'_datasize_{ds_size}'\n",
    "\n",
    "wandb.log({'metrics': metrics,\n",
    "            'losses': losses})\n",
    "\n",
    "# Train the model\n",
    "train_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor='val_loss',\n",
    "        save_best_only=True, verbose=1,\n",
    "        save_weights_only=False),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7), WandbCallback()\n",
    "]\n",
    "\n",
    "loss_history = tm.training_model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
