{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "# import wandb\n",
    "import errno\n",
    "import deepcell\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from deepcell.data.tracking import prepare_dataset\n",
    "from deepcell.data.tracking import Track, concat_tracks\n",
    "from deepcell.model_zoo.tracking import GNNTrackingModel\n",
    "from deepcell.utils.tracking_utils import trks_stats, load_trks\n",
    "\n",
    "from tensorflow_addons.optimizers import RectifiedAdam as RAdam\n",
    "\n",
    "# from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1 GPUs\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU count\n",
    "from deepcell import train_utils\n",
    "num_gpus = train_utils.count_gpus()\n",
    "print('Training on {} GPUs'.format(num_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup directories\n",
    "ROOT_DIR = '/data/tracking_data'  # TODO: Change this! Usually a mounted volume\n",
    "\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models'))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs'))\n",
    "DATA_DIR = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "OUTPUT_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'nuc_tracking'))\n",
    "\n",
    "# create directories if they do not exist\n",
    "for d in (MODEL_DIR, LOG_DIR, OUTPUT_DIR):\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and view stats on this file\n",
    "filename = 'train.trks'\n",
    "path = os.path.join('../trk_data/',filename)\n",
    "trks_data = load_trks(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tracks = Track(tracked_data=trks_data)\n",
    "# track_info = concat_tracks([all_tracks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = os.path.abspath(os.path.join(ROOT_DIR, 'dataset_idxs_dvc.npy'))\n",
    "dataset_indicies = np.load(dataset_sizes, allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepcell.datasets.tracked import hek293\n",
    "\n",
    "# filename = 'sample_tracking.trks'\n",
    "# (X_train, y_train), (X_test, y_test) = hek293.load_tracked_data(filename)\n",
    "# path = os.path.join(DATA_DIR, filename)\n",
    "# trks_data = load_trks(path)\n",
    "# all_tracks = Track(tracked_data=trks_data)\n",
    "# track_info = concat_tracks([all_tracks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for metrics\n",
    "\"\"\"\n",
    "def filter_and_flatten(y_true, y_pred):\n",
    "    n_classes = tf.shape(y_true)[-1]\n",
    "    new_shape = [-1, n_classes]\n",
    "    y_true = tf.reshape(y_true, new_shape)\n",
    "    y_pred = tf.reshape(y_pred, new_shape)\n",
    "\n",
    "    # Mask out the padded cells\n",
    "    y_true_reduced = tf.reduce_sum(y_true, axis=-1)\n",
    "    good_loc = tf.where(y_true_reduced == 1)[:, 0]\n",
    "\n",
    "    y_true = tf.gather(y_true, good_loc, axis=0)\n",
    "    y_pred = tf.gather(y_pred, good_loc, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "class Recall(tf.keras.metrics.Recall):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Recall, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class Precision(tf.keras.metrics.Precision):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Precision, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "    return deepcell.losses.weighted_categorical_crossentropy(\n",
    "        y_true, y_pred,\n",
    "        n_classes=tf.shape(y_true)[-1],\n",
    "        axis=-1)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = RAdam(learning_rate=1e-3, clipnorm=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "losses = {'temporal_adj_matrices': loss_function}\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    Recall(class_id=0, name='same_recall'),\n",
    "    Recall(class_id=1, name='different_recall'),\n",
    "    Recall(class_id=2, name='daughter_recall'),\n",
    "    Precision(class_id=0, name='same_precision'),\n",
    "    Precision(class_id=1, name='different_precision'),\n",
    "    Precision(class_id=2, name='daughter_precision'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data idx 0 size 22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:48<00:00,  2.19s/it]\n",
      "100%|██████████| 22/22 [00:42<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "seed = 1   # random seed for training/validation data split\n",
    "batch_size = 1\n",
    "track_length = 8  # only train on 8 frames at once\n",
    "val_size = .20  # % of data saved as validation\n",
    "test_size = .1  # % of data held out as a test set\n",
    "n_epochs = 2  # number of training epochs\n",
    "\n",
    "# steps_per_epoch = 1000\n",
    "# validation_steps = 200\n",
    "steps_per_epoch = 5\n",
    "validation_steps = 2\n",
    "\n",
    "translation_range = 512 #X_train.shape[-2]\n",
    "\n",
    "n_layers = 1 # number of graph convolutions\n",
    "\n",
    "\n",
    "# for i in range(len(dataset_indicies)):\n",
    "i=0\n",
    "new_data = {}\n",
    "new_data['lineages'] = list(np.array(trks_data['lineages'])[dataset_indicies[i]])\n",
    "new_data['X'] = trks_data['X'][dataset_indicies[i],...]\n",
    "new_data['y'] = trks_data['y'][dataset_indicies[i],...]\n",
    "\n",
    "ds_size = len(dataset_indicies[i])\n",
    "\n",
    "print()\n",
    "print('data idx', i, 'size', ds_size)\n",
    "print()\n",
    "\n",
    "all_tracks = Track(tracked_data=new_data)\n",
    "track_info = concat_tracks([all_tracks])\n",
    "\n",
    "# find maximum number of cells in any frame\n",
    "max_cells = track_info['appearances'].shape[2]\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = prepare_dataset(\n",
    "    track_info,\n",
    "    rotation_range=180,\n",
    "    translation_range=translation_range,\n",
    "    seed=seed,\n",
    "    val_size=val_size,\n",
    "    test_size=test_size,\n",
    "    batch_size=batch_size,\n",
    "    track_length=track_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.7199 - same_recall: 0.3873 - different_recall: 0.0554 - daughter_recall: 0.3333 - same_precision: 0.0060 - different_precision: 0.9827 - daughter_precision: 9.1943e-05WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa97a113a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa97a113a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 34s 2s/step - loss: 0.7199 - same_recall: 0.3873 - different_recall: 0.0554 - daughter_recall: 0.3333 - same_precision: 0.0060 - different_precision: 0.9827 - daughter_precision: 9.1943e-05 - val_loss: 0.9438 - val_same_recall: 0.6614 - val_different_recall: 0.0000e+00 - val_daughter_recall: 0.0000e+00 - val_same_precision: 0.0255 - val_different_precision: 0.0000e+00 - val_daughter_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.94384, saving model to /data/tracking_data/models/graph_tracking_model_seed1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_9_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/tracking_data/models/graph_tracking_model_seed1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/tracking_data/models/graph_tracking_model_seed1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.4907 - same_recall: 0.2973 - different_recall: 0.3307 - daughter_recall: 0.2500 - same_precision: 0.0262 - different_precision: 0.9750 - daughter_precision: 4.0833e-04 - val_loss: 1.8695 - val_same_recall: 0.9048 - val_different_recall: 0.0000e+00 - val_daughter_recall: 0.0000e+00 - val_same_precision: 0.0133 - val_different_precision: 0.0000e+00 - val_daughter_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.94384\n",
      "\n",
      "finished training\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_9_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/tracking_data/models/TrackingModelInf_se2c_datasize_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/tracking_data/models/TrackingModelInf_se2c_datasize_5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/tracking_data/models/TrackingModelNE_se2c_datasize_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/tracking_data/models/TrackingModelNE_se2c_datasize_5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "finished this shit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# seed = 1   # random seed for training/validation data split\n",
    "# batch_size = 4\n",
    "# track_length = 8  # only train on 8 frames at once\n",
    "# val_size = .20  # % of data saved as validation\n",
    "# test_size = .1  # % of data held out as a test set\n",
    "# n_epochs = 1  # number of training epochs\n",
    "\n",
    "# # steps_per_epoch = 1000\n",
    "# # validation_steps = 200\n",
    "# steps_per_epoch = 10\n",
    "# validation_steps = 2\n",
    "\n",
    "# translation_range = 512 #X_train.shape[-2]\n",
    "\n",
    "# n_layers = 1 # number of graph convolutions\n",
    "\n",
    "# model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "# model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "# # for i in range(len(dataset_indicies)):\n",
    "# i=0\n",
    "# new_data = {}\n",
    "# new_data['lineages'] = list(np.array(trks_data['lineages'])[dataset_indicies[i]])\n",
    "# new_data['X'] = trks_data['X'][dataset_indicies[i],...]\n",
    "# new_data['y'] = trks_data['y'][dataset_indicies[i],...]\n",
    "\n",
    "# ds_size = len(dataset_indicies[i])\n",
    "\n",
    "# print()\n",
    "# print('data idx', i, 'size', ds_size)\n",
    "# print()\n",
    "\n",
    "# all_tracks = Track(tracked_data=new_data)\n",
    "# track_info = concat_tracks([all_tracks])\n",
    "\n",
    "# # find maximum number of cells in any frame\n",
    "# max_cells = track_info['appearances'].shape[2]\n",
    "\n",
    "\n",
    "# model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "# model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "\n",
    "# train_data, val_data, test_data = prepare_dataset(\n",
    "#     track_info,\n",
    "#     rotation_range=180,\n",
    "#     translation_range=translation_range,\n",
    "#     seed=seed,\n",
    "#     val_size=val_size,\n",
    "#     test_size=test_size,\n",
    "#     batch_size=batch_size,\n",
    "#     track_length=track_length)\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "graph_layer = 'se2c'  # type of graph convolution layer\n",
    "# for graph_layer in ['se2t', 'gcn', 'se2c', 'gcs']:\n",
    "\n",
    "tm = GNNTrackingModel(max_cells=max_cells, n_layers=n_layers, graph_layer=graph_layer)\n",
    "\n",
    "# Compile model\n",
    "tm.training_model.compile(optimizer=optimizer, loss=losses, metrics=metrics)\n",
    "\n",
    "layer = graph_layer\n",
    "ds_size = 5\n",
    "\n",
    "train_log = os.path.join(ROOT_DIR, f'train_logs/training_log_{layer}_{ds_size}.csv')\n",
    "csv_logger = CSVLogger(train_log)\n",
    "\n",
    "# run = wandb.init(project='testing_new', reinit=True)\n",
    "# wandb.run.name = layer+f'_datasize_{ds_size}'\n",
    "\n",
    "# wandb.log({'metrics': metrics,\n",
    "#             'losses': losses})\n",
    "\n",
    "# Train the model\n",
    "train_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor='val_loss',\n",
    "        save_best_only=True, verbose=1,\n",
    "        save_weights_only=False),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7), csv_logger]\n",
    "\n",
    "\n",
    "loss_history = tm.training_model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=train_callbacks)\n",
    "\n",
    "print()\n",
    "print('finished training')\n",
    "print()\n",
    "\n",
    "\n",
    "# Save models for prediction\n",
    "inf_path = os.path.join(MODEL_DIR, f'TrackingModelInf_{layer}_datasize_{ds_size}')\n",
    "ne_path = os.path.join(MODEL_DIR, f'TrackingModelNE_{layer}_datasize_{ds_size}')\n",
    "\n",
    "tm.inference_model.save(inf_path)\n",
    "tm.neighborhood_encoder.save(ne_path)\n",
    "\n",
    "# run.finish()\n",
    "\n",
    "print()\n",
    "print('finished this shit')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>daughter_precision</th>\n",
       "      <th>daughter_recall</th>\n",
       "      <th>different_precision</th>\n",
       "      <th>different_recall</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>same_precision</th>\n",
       "      <th>same_recall</th>\n",
       "      <th>val_daughter_precision</th>\n",
       "      <th>val_daughter_recall</th>\n",
       "      <th>val_different_precision</th>\n",
       "      <th>val_different_recall</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_same_precision</th>\n",
       "      <th>val_same_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.982661</td>\n",
       "      <td>0.055404</td>\n",
       "      <td>0.719878</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.387300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.025482</td>\n",
       "      <td>0.661417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.330657</td>\n",
       "      <td>0.490691</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.869506</td>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  daughter_precision  daughter_recall  different_precision  \\\n",
       "0      0            0.000092         0.333333             0.982661   \n",
       "1      1            0.000408         0.250000             0.975007   \n",
       "\n",
       "   different_recall      loss     lr  same_precision  same_recall  \\\n",
       "0          0.055404  0.719878  0.001        0.005998     0.387300   \n",
       "1          0.330657  0.490691  0.001        0.026163     0.297297   \n",
       "\n",
       "   val_daughter_precision  val_daughter_recall  val_different_precision  \\\n",
       "0                     0.0                  0.0                      0.0   \n",
       "1                     0.0                  0.0                      0.0   \n",
       "\n",
       "   val_different_recall  val_loss  val_same_precision  val_same_recall  \n",
       "0                   0.0  0.943835            0.025482         0.661417  \n",
       "1                   0.0  1.869506            0.013350         0.904762  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NV_GPU=2 nvidia-docker run -it --entrypoint python3 -v $PWD/deepcell:/usr/local/lib/python3.6/dist-packages/deepcell/ -v $PWD/deepcell:/deepcell -v $PWD/notebooks:/notebooks -v /$PWD/../data:/data -v /deepcell_data/users/uriah/data-registry/data/training/tracking-nuclear/:/trk_data $USER/equiv_networks:latest /notebooks/training/tracking/train_tracking_models.py > logfiles.txt 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1   # random seed for training/validation data split\n",
    "batch_size = 4\n",
    "track_length = 8  # only train on 8 frames at once\n",
    "val_size = .20  # % of data saved as validation\n",
    "test_size = .1  # % of data held out as a test set\n",
    "n_epochs = 12  # number of training epochs\n",
    "\n",
    "steps_per_epoch = 1000\n",
    "validation_steps = 200\n",
    "\n",
    "n_layers = 1  # number of graph convolutions\n",
    "# graph_layer = 'se2c'  # type of graph convolution layer\n",
    "\n",
    "translation_range = 512 #X_train.shape[-2]\n",
    "\n",
    "model_name = 'graph_tracking_model_seed{}'.format(seed)\n",
    "model_path = os.path.join(MODEL_DIR, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_flatten(y_true, y_pred):\n",
    "    n_classes = tf.shape(y_true)[-1]\n",
    "    new_shape = [-1, n_classes]\n",
    "    y_true = tf.reshape(y_true, new_shape)\n",
    "    y_pred = tf.reshape(y_pred, new_shape)\n",
    "\n",
    "    # Mask out the padded cells\n",
    "    y_true_reduced = tf.reduce_sum(y_true, axis=-1)\n",
    "    good_loc = tf.where(y_true_reduced == 1)[:, 0]\n",
    "\n",
    "    y_true = tf.gather(y_true, good_loc, axis=0)\n",
    "    y_pred = tf.gather(y_pred, good_loc, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "class Recall(tf.keras.metrics.Recall):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Recall, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class Precision(tf.keras.metrics.Precision):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super(Precision, self).update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "    return deepcell.losses.weighted_categorical_crossentropy(\n",
    "        y_true, y_pred,\n",
    "        n_classes=tf.shape(y_true)[-1],\n",
    "        axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.optimizers import RectifiedAdam as RAdam\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = RAdam(learning_rate=1e-3, clipnorm=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "losses = {'temporal_adj_matrices': loss_function}\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    Recall(class_id=0, name='same_recall'),\n",
    "    Recall(class_id=1, name='different_recall'),\n",
    "    Recall(class_id=2, name='daughter_recall'),\n",
    "    Precision(class_id=0, name='same_precision'),\n",
    "    Precision(class_id=1, name='different_precision'),\n",
    "    Precision(class_id=2, name='daughter_precision'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data idx 0 size 22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:46<00:00,  2.10s/it]\n",
      "100%|██████████| 22/22 [00:41<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "se2t\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jwa8261a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 253... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137c7d3e72cf4a4ca3d0f5a76c5d602f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">nightmarish-sorcery-4</strong>: <a href=\"https://wandb.ai/ulisrael/cell_tracking/runs/jwa8261a\" target=\"_blank\">https://wandb.ai/ulisrael/cell_tracking/runs/jwa8261a</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211031_001451-jwa8261a/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jwa8261a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ulisrael/cell_tracking/runs/1d6q4e7n\" target=\"_blank\">spooky-reaper-2</a></strong> to <a href=\"https://wandb.ai/ulisrael/cell_tracking\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset_indicies)):\n",
    "\n",
    "    new_data = {}\n",
    "    new_data['lineages'] = list(np.array(trks_data['lineages'])[dataset_indicies[i]])\n",
    "    new_data['X'] = trks_data['X'][dataset_indicies[i],...]\n",
    "    new_data['y'] = trks_data['y'][dataset_indicies[i],...]\n",
    "    \n",
    "    ds_size = len(dataset_indicies[i])\n",
    "\n",
    "    print()\n",
    "    print('data idx', i, 'size', ds_size)\n",
    "    print()\n",
    "\n",
    "    all_tracks = Track(tracked_data=new_data)\n",
    "    track_info = concat_tracks([all_tracks])\n",
    "\n",
    "    max_cells = track_info['appearances'].shape[2]\n",
    "\n",
    "    train_data, val_data, test_data = prepare_dataset(\n",
    "        track_info,\n",
    "        rotation_range=180,\n",
    "        translation_range=translation_range,\n",
    "        seed=seed,\n",
    "        val_size=val_size,\n",
    "        test_size=test_size,\n",
    "        batch_size=batch_size,\n",
    "        track_length=track_length)\n",
    "\n",
    "    graph_layers = ['se2t', 'gcn', 'se2c', 'gcs']\n",
    "    for layer in graph_layers:\n",
    "        print()\n",
    "        print(layer)\n",
    "        print()\n",
    "\n",
    "        run = wandb.init(project='cell_tracking', reinit=True)\n",
    "        wandb.run.name = layer+f'_datasize_{ds_size}'\n",
    "\n",
    "        tm = GNNTrackingModel(max_cells=max_cells, n_layers=n_layers, graph_layer=layer)\n",
    "\n",
    "        wandb.log({'metrics': metrics,\n",
    "                   'losses': losses})\n",
    "\n",
    "        tm.training_model.compile(optimizer=optimizer, loss=losses, metrics=metrics)\n",
    "\n",
    "        # Train the model\n",
    "        train_callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                model_path, monitor='val_loss',\n",
    "                save_best_only=True, verbose=1,\n",
    "                save_weights_only=False),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, verbose=1,\n",
    "                patience=3, min_lr=1e-7), WandbCallback()\n",
    "        ]\n",
    "\n",
    "        loss_history = tm.training_model.fit(\n",
    "            train_data,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=val_data,\n",
    "            validation_steps=validation_steps,\n",
    "            epochs=n_epochs,\n",
    "            verbose=1,\n",
    "            callbacks=train_callbacks)\n",
    "\n",
    "\n",
    "        # Save models for prediction\n",
    "        inf_path = os.path.join(MODEL_DIR, f'TrackingModelInf_{layer}_datasize_{ds_size}')\n",
    "        ne_path = os.path.join(MODEL_DIR, f'TrackingModelNE_{layer}_datasize_{ds_size}')\n",
    "\n",
    "        tm.inference_model.save(inf_path)\n",
    "        tm.neighborhood_encoder.save(ne_path)\n",
    "\n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Verify GPU count\n",
    "from deepcell import train_utils\n",
    "num_gpus = train_utils.count_gpus()\n",
    "print('Training on {} GPUs'.format(num_gpus))\n",
    "\n",
    "layer = graph_layer\n",
    "\n",
    "run = wandb.init(project='cell_tracking', reinit=True)\n",
    "wandb.run.name = layer+f'_datasize_{ds_size}'\n",
    "\n",
    "wandb.log({'metrics': metrics,\n",
    "            'losses': losses})\n",
    "\n",
    "# Train the model\n",
    "train_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor='val_loss',\n",
    "        save_best_only=True, verbose=1,\n",
    "        save_weights_only=False),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7), WandbCallback()\n",
    "]\n",
    "\n",
    "loss_history = tm.training_model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
