{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplex Model Training with Hyper Parameter Search\n",
    "\n",
    "Using `optuna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna config\n",
    "TRIALS = 25  # number of HPS trials to run.\n",
    "TIMEOUT = 3600 * 24  # time limit for HPS in seconds.\n",
    "\n",
    "# Training paramters\n",
    "SEED = 0\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# File paths for data and models\n",
    "NPZ_NAME = '20200819_multiplex_normalized_512x512'\n",
    "EXP_NAME = '20200819_hyper_parameter'\n",
    "MODEL_NAME = '{}_deep_watershed'.format(NPZ_NAME)\n",
    "\n",
    "ROOT_DIR = '/data'\n",
    "LOG_DIR = os.path.join(ROOT_DIR, 'logs')\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models', EXP_NAME)\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'users/willgraf/mibi-hps')\n",
    "\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, '{}_train.npz'.format(NPZ_NAME))\n",
    "TEST_DATA_FILE = os.path.join(DATA_DIR, '{}_test.npz'.format(NPZ_NAME))\n",
    "VAL_DATA_FILE = os.path.join(DATA_DIR, '{}_val.npz'.format(NPZ_NAME))\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_data = np.load(os.path.join(DATA_DIR, TRAIN_DATA_FILE))\n",
    "    X_train = train_data['X']\n",
    "    y_train = train_data['y']\n",
    "    \n",
    "    val_data = np.load(os.path.join(DATA_DIR, VAL_DATA_FILE))\n",
    "    X_val = val_data['X']\n",
    "    y_val = val_data['y']\n",
    "\n",
    "#     test_data = np.load(os.path.join(DATA_DIR, TEST_DATA_FILE))\n",
    "#     X_test = test_data['X']\n",
    "#     y_test = test_data['y']\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import losses\n",
    "from deepcell.model_zoo.panopticnet import PanopticNet\n",
    "\n",
    "\n",
    "def semantic_loss(n_classes):\n",
    "    def _semantic_loss(y_pred, y_true):\n",
    "        if n_classes > 1:\n",
    "            return 0.01 * losses.weighted_categorical_crossentropy(\n",
    "                y_pred, y_true, n_classes=n_classes)\n",
    "        return tf.keras.losses.MSE(y_pred, y_true)\n",
    "    return _semantic_loss\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    \n",
    "    model = PanopticNet(\n",
    "        backbone='resnet50',\n",
    "        input_shape=(256, 256, 2),\n",
    "        norm_method=None,\n",
    "        num_semantic_heads=4,\n",
    "        num_semantic_classes=[1, 1, 2, 3], # inner distance, outer distance, fgbg, pixelwise\n",
    "        location=True,  # should always be true\n",
    "        include_top=True)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
    "    clipnorm = .001\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr, clipnorm=clipnorm)\n",
    "    \n",
    "    loss = {}\n",
    "\n",
    "    # Give losses for all of the semantic heads\n",
    "    for layer in model.layers:\n",
    "        if layer.name.startswith('semantic_'):\n",
    "            n_classes = layer.output_shape[-1]\n",
    "            loss[layer.name] = semantic_loss(n_classes)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import relabel_sequential\n",
    "\n",
    "from deepcell.image_generators import CroppingDataGenerator\n",
    "\n",
    "\n",
    "def create_data_generators(trial, train_dict, val_dict):\n",
    "    # Tunable parameters\n",
    "    # preprocessing parameter\n",
    "    k = trial.suggest_int('k', 32, 256)\n",
    "\n",
    "    # data generator parameter\n",
    "    min_objects = trial.suggest_int('min_objects', 1, 50)\n",
    "\n",
    "    # transform parameters\n",
    "    inner_erosion = trial.suggest_int('inner_erosion', 0, 5)\n",
    "    outer_erosion = trial.suggest_int('outer_erosion', 0, 5)\n",
    "    dilation_radius = trial.suggest_int('dilation_radius', 0, 5)\n",
    "    \n",
    "    # data augmentation parameters\n",
    "    zoom_min = trial.suggest_float('zoom_min', 0.25, 1.)\n",
    "    zoom_max = trial.suggest_float('zoom_max', 1., 4.)\n",
    "\n",
    "    fill_modes = ['constant', 'nearest', 'reflect', 'wrap']\n",
    "    fill_mode = trial.suggest_categorical('fill_mode', fill_modes)\n",
    "\n",
    "    X_train, y_train = train_dict['X'], train_dict['y']\n",
    "    X_val, y_val = val_dict['X'], val_dict['y']\n",
    "\n",
    "    # Preprocess the data\n",
    "    train_dict['X'] = phase_preprocess(train_dict['X'], k)\n",
    "    val_dict['X'] = phase_preprocess(val_dict['X'], k)\n",
    "\n",
    "    # use augmentation for training but not validation\n",
    "    datagen = CroppingDataGenerator(\n",
    "        fill_mode=fill_mode,\n",
    "        rotation_range=180,\n",
    "        shear_range=0,\n",
    "        zoom_range=(zoom_min, zoom_max),\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        crop_size=(256, 256))\n",
    "\n",
    "    datagen_val = CroppingDataGenerator(\n",
    "        fill_mode=fill_mode,\n",
    "        rotation_range=0,\n",
    "        shear_range=0,\n",
    "        zoom_range=0,\n",
    "        horizontal_flip=0,\n",
    "        vertical_flip=0,\n",
    "        crop_size=(256, 256))\n",
    "\n",
    "    transforms = ['inner-distance', 'watershed-cont', 'fgbg', 'pixelwise']\n",
    "\n",
    "    transforms_kwargs = {\n",
    "        'watershed-cont': {\n",
    "            'erosion_width': inner_erosion\n",
    "        },\n",
    "        'pixelwise': {\n",
    "            'dilation_radius': dilation_radius\n",
    "        },\n",
    "        'inner-distance': {\n",
    "            'erosion_width': outer_erosion,\n",
    "            'alpha': 'auto'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    train_data = datagen.flow(\n",
    "        train_dict,\n",
    "        seed=SEED,\n",
    "        transforms=transforms,\n",
    "        transforms_kwargs=transforms_kwargs,\n",
    "        min_objects=min_objects,\n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "    val_data = datagen_val.flow(\n",
    "        val_dict,\n",
    "        seed=SEED,\n",
    "        transforms=transforms,\n",
    "        transforms_kwargs=transforms_kwargs,\n",
    "        min_objects=min_objects,\n",
    "        batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.train_utils import count_gpus, get_callbacks, rate_scheduler\n",
    "from deepcell_toolbox.processing import phase_preprocess\n",
    "\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Clear clutter from previous TensorFlow graphs.\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    monitor = 'val_loss'\n",
    "    \n",
    "    model_path = os.path.join(MODEL_DIR, '{}.h5'.format(MODEL_NAME))\n",
    "    loss_path = os.path.join(MODEL_DIR, '{}.npz'.format(MODEL_NAME))\n",
    "\n",
    "    # Create model instance.\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # load the data\n",
    "    (X_train, y_train), (X_val, y_val) = load_data()\n",
    "\n",
    "    train_dict = {'X': X_train, 'y': y_train}\n",
    "    val_dict = {'X': X_val, 'y': y_val}\n",
    "\n",
    "    # Create dataset instance.\n",
    "    train_data, val_data = create_data_generators(trial, train_dict, val_dict)\n",
    "\n",
    "    # Create callbacks for early stopping and pruning.\n",
    "    train_callbacks = get_callbacks(\n",
    "        model_path,\n",
    "        lr_sched=rate_scheduler(lr=1e-4, decay=0.99),\n",
    "        tensorboard_log_dir=LOG_DIR,\n",
    "        save_weights_only=count_gpus() >= 2,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "    \n",
    "    train_callbacks.append(TFKerasPruningCallback(trial, monitor))\n",
    "    train_callbacks.append(tf.keras.callbacks.EarlyStopping(patience=5))\n",
    "\n",
    "    # Train model.\n",
    "    history = model.fit_generator(\n",
    "        train_data,\n",
    "        steps_per_epoch=train_data.y.shape[0] // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=val_data.y.shape[0] // BATCH_SIZE,\n",
    "        callbacks=train_callbacks)\n",
    "\n",
    "    return history.history[monitor][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(study):\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print('Study statistics: ')\n",
    "    print('  Number of finished trials: ', len(study.trials))\n",
    "    print('  Number of pruned trials: ', len(pruned_trials))\n",
    "    print('  Number of complete trials: ', len(complete_trials))\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('  Value: ', trial.value)\n",
    "\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=2)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=TRIALS, timeout=TIMEOUT)\n",
    "\n",
    "show_result(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
